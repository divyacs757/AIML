{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5621c648-863b-4e2e-ba36-9bc4cd66abf0",
   "metadata": {},
   "source": [
    "# Experiment-13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7be7e2-d0dd-430f-bf52-97549b349852",
   "metadata": {},
   "source": [
    "### Write a NLP program to demonstrat following tasks\n",
    "### a)Tokenization, Removal of Stopword, Removal of punctuations,POS and NER Tagging\n",
    "### b)to demonstrate Bag of words,TF & IDF Vectrization, N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aba36-60d8-4d47-a2a4-0ca11ab4f3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cb71aa-f13e-4dc6-bd0c-a7546de33baa",
   "metadata": {},
   "source": [
    "### a)Tokenization, Removal of Stopword, Removal of punctuations,POS and NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39841843-7f55-4da8-9cdb-711adeff81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag, ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b52baf-7ac2-45c1-9338-d1061275a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4bba49-0599-46c8-ae4c-d66e0236cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is looking at buying U.K. startup for $1 billion in 2025.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e68ea6e3-c51e-4bc3-9213-5a05cb7651d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', 'in', '2025']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization → Breaking text into words\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text1)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f0909a-8b9c-4b8f-bf73-704602aac1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stopword Removal: ['Apple', 'looking', 'buying', 'startup', 'billion']\n"
     ]
    }
   ],
   "source": [
    "#Stopword Removal → Filtering out common words\n",
    "# Stopword Removal\n",
    "filtered = [w for w in tokens if w.isalpha() and w.lower() not in stopwords.words(\"english\")]\n",
    "print(\"After Stopword Removal:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f978a93b-11ec-4757-813c-e49ea4d1a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Punctuation Removal: ['Apple', 'looking', 'buying', 'startup', 'billion']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuation (just in case any remain)\n",
    "tokens_no_punct = [w for w in filtered if w not in string.punctuation]\n",
    "\n",
    "print(\"After Punctuation Removal:\", tokens_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e158f4ef-cab5-447d-9f07-56dffa2a2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Apple', 'NNP'), ('looking', 'VBG'), ('buying', 'VBG'), ('startup', 'NN'), ('billion', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging → Identifying word roles (noun, verb, etc.)\n",
    "# POS Tagging(Part of Speech)\n",
    "print(\"POS Tags:\", pos_tag(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc704aef-64eb-44da-9a54-ef960b732511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY'), ('2025', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "#NER → Extracting entities like names, dates, money\n",
    "# Named Entity Recognition\n",
    "doc = nlp(text)\n",
    "print(\"NER:\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993d74a-e39c-4740-a6f8-7df5aa0998b8",
   "metadata": {},
   "source": [
    "## b)to demonstrate Bag of words,TF & IDF Vectrization, N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b30997-7ccb-4d77-9a9d-644b00dc6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports ----\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa227b4-8875-49dd-a2b8-edd9e4abbb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b725c4-22be-4530-80b5-74c76f97ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text1 =\"Apple is looking at buying U.K. startup for $1 billion in 2025\"\n",
    "text2=\"startup for $1 billion in 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c069c67e-5818-46fe-9ace-787b19c9f6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple is looking at buying U.K. startup for $1 billion in 2025'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb57853b-2d27-49b5-8b47-768d441b2d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', 'in', '2025']\n"
     ]
    }
   ],
   "source": [
    "# ---- 1. Tokenization ----\n",
    "tokens = word_tokenize(text1)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1fb1109-1c8e-43e7-8c1d-26a6329e45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BoW Words: ['2025' 'apple' 'at' 'billion' 'buying' 'for' 'in' 'is' 'looking'\n",
      " 'startup']\n",
      "BoW Counts: [[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 1 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- 2. Bag of Words (BoW) ----\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([text1,text2])\n",
    "print(\"\\nBoW Words:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Counts:\", bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dcf7819-2b57-44d5-8f2e-20b83b9ddecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Words: ['2025' 'apple' 'at' 'billion' 'buying' 'for' 'in' 'is' 'looking'\n",
      " 'startup']\n",
      "TF-IDF Values: [[0.25926702 0.36439074 0.36439074 0.25926702 0.36439074 0.25926702\n",
      "  0.25926702 0.36439074 0.36439074 0.25926702]\n",
      " [0.4472136  0.         0.         0.4472136  0.         0.4472136\n",
      "  0.4472136  0.         0.         0.4472136 ]]\n"
     ]
    }
   ],
   "source": [
    "# ---- 3. TF-IDF ----\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform([text1,text2])\n",
    "print(\"\\nTF-IDF Words:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Values:\", tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4368ff40-cd2f-47d1-8377-a5a926b47edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('Apple', 'is'), ('is', 'looking'), ('looking', 'at'), ('at', 'buying'), ('buying', 'U.K.'), ('U.K.', 'startup'), ('startup', 'for'), ('for', '$'), ('$', '1'), ('1', 'billion'), ('billion', 'in'), ('in', '2025')]\n",
      "Trigrams: [('Apple', 'is', 'looking'), ('is', 'looking', 'at'), ('looking', 'at', 'buying'), ('at', 'buying', 'U.K.'), ('buying', 'U.K.', 'startup'), ('U.K.', 'startup', 'for'), ('startup', 'for', '$'), ('for', '$', '1'), ('$', '1', 'billion'), ('1', 'billion', 'in'), ('billion', 'in', '2025')]\n"
     ]
    }
   ],
   "source": [
    "#N-grams → Generating word pairs/triples\n",
    "# N-grams\n",
    "print(\"Bigrams:\", list(ngrams(tokens, 2)))\n",
    "print(\"Trigrams:\", list(ngrams(tokens, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d091184-5e0c-48c6-99f2-0a3ec66cdc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d129c2-f9a7-4ea6-9709-d0d319325d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
